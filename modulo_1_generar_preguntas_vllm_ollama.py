#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Genera dos conjuntos de datos sint√©ticos (pregunta-respuesta y metadatos) a partir de un directorio
# de documentos usando DeepEval y un servidor vLLM local (OpenAI-compatible).

Requisitos:
    pip install deepeval vllm pandas

Antes de ejecutar, aseg√∫rate de lanzar vLLM:
    python -m vllm.entrypoints.openai.api_server \
        --model /ruta/al/modelo \
        --port 8000 \
        --dtype float16

Autor: Alberto G. Garc√≠a
Fecha: 2025-04-24
"""

import os
import glob
from pathlib import Path
import pandas as pd
from deepeval.synthesizer import Synthesizer
from deepeval.models import LocalModel
from deepeval.synthesizer.config import StylingConfig, FiltrationConfig, EvolutionConfig, Evolution, ContextConstructionConfig 

# Persistir base de vectores para acelerar embeddings
os.environ["DEEPEVAL_PRESERVE_VECTOR_DB"] = "1"

# ----------------------------------------------------------------------------
# 1. Rutas de entrada / salida
# ----------------------------------------------------------------------------
DOCUMENTS_DIR = Path("/home/jovyan/Documentos/Docs_pdf")
OUTPUT_DIR   = Path("/home/jovyan/DEEPEVAL_AL/output")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
DATASET_FILE  = OUTPUT_DIR / "2_dataset.csv"

# ----------------------------------------------------------------------------
# 2. Conexi√≥n al modelo vLLM
# ----------------------------------------------------------------------------
os.environ.setdefault("OPENAI_API_KEY", "EMPTY")  # vLLM ignora la clave
vllm_model = LocalModel(
    model="NousResearch/Meta-Llama-3-8B-Instruct",
    #model="meta-llama/Llama-3.1-8B-Instruct",
    #base_url="http://localhost:8000/v1/",
    #openai_api_key=os.environ["OPENAI_API_KEY"]
)
print(f"ü¶ô Modelo vLLM configurado: {vllm_model.get_model_name()}")

# ----------------------------------------------------------------------------
# 3. Configuraci√≥n de Filtrado
# ----------------------------------------------------------------------------
filtration_cfg = FiltrationConfig(
    synthetic_input_quality_threshold= 0.5,
    max_quality_retries= 3,
    #critic_model= vllm_model
)

# ----------------------------------------------------------------------------
# 4. Configuraci√≥n de Evoluci√≥n
# ----------------------------------------------------------------------------
evolution_cfg = EvolutionConfig(
    num_evolutions=1, #modificar hasta 3 para aumentar la complejidad
    evolutions={
        Evolution.REASONING:    1 / 7,  # Razonamiento l√≥gico
        Evolution.MULTICONTEXT: 1 / 7,  # Multi-contexto
        Evolution.CONCRETIZING: 1 / 7,  # Concretizar detalle
        Evolution.CONSTRAINED:  1 / 7,  # Restringir l√≠mites
        Evolution.COMPARATIVE:  1 / 7,  # Comparativo (peso total)
        Evolution.HYPOTHETICAL: 1 / 7,  # Hipot√©tico
        Evolution.IN_BREADTH:   1 / 7   # Cobertura amplia
    }
)

# ----------------------------------------------------------------------------
# 5. Configuraci√≥n de Estilo
# ----------------------------------------------------------------------------
estilo_es = StylingConfig(
    input_format=(
        "Genera preguntas concisas EN ESPA√ëOL que puedan responderse exclusivamente con la informaci√≥n del contexto proporcionado."
    ),
    expected_output_format="Respuesta corta y breve en ESPA√ëOL. Responde UNICAMENTE con la respuesta, sin a√±adir comentarios.",
    task="Responder consultas sobre los documentos, en ESPA√ëOL.",
    scenario="Evaluaci√≥n de comprensi√≥n de documentos en ESPA√ëOL."
)

# ----------------------------------------------------------------------------
# 6. Creaci√≥n del Synthesizer
# ----------------------------------------------------------------------------
synthesizer = Synthesizer(
    model=vllm_model,
    async_mode=False,
    max_concurrent=5,
    filtration_config=filtration_cfg,
    evolution_config=evolution_cfg,
    styling_config=estilo_es,
    cost_tracking=True
)

# ----------------------------------------------------------------------------
# 7. Carga de documentos
# ----------------------------------------------------------------------------
document_paths = []
for ext in ("*.txt", "*.pdf", "*.docx"):
    document_paths += glob.glob(str(DOCUMENTS_DIR / ext))
print(f"üìÑ Documentos encontrados: {len(document_paths)}")

# ----------------------------------------------------------------------------
# 8. Generaci√≥n de Goldens
# ----------------------------------------------------------------------------
context_construction_cfg = ContextConstructionConfig(
    #embedder: Optional[Union[str, DeepEvalBaseEmbeddingModel]] = None,
    critic_model= vllm_model,
    #encoding: Optional[str] = None,
    max_contexts_per_document= 3,
    min_contexts_per_document= 1,
    max_context_length= 3,
    min_context_length= 1,
    chunk_size= 1024,
    chunk_overlap= 0,
    context_quality_threshold= 0.5,
    context_similarity_threshold= 0.0,
    max_retries= 3
)
    

synthesizer.generate_goldens_from_docs(
    document_paths=document_paths,
    include_expected_output=True,
    max_goldens_per_context=4,
    context_construction_config=context_construction_cfg
    
)
print(f"‚úÖ Goldens generados: {len(synthesizer.synthetic_goldens)}")

# Guardar dataset principal
df = synthesizer.to_pandas()
df.to_csv(DATASET_FILE, index=False, encoding='utf-8')
print(f"üíæ Dataset guardado en: {DATASET_FILE}")
